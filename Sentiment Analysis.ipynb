{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeled Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = \"data\"\n",
    "positive_words = []\n",
    "negative_words = []\n",
    "neutral_words = []\n",
    "\n",
    "for directories, subdirs, files in os.walk(rootdir):\n",
    "    if (os.path.split(directories)[1] == '1' or os.path.split(directories)[1] == '2' or os.path.split(directories)[1] == '3' or os.path.split(directories)[1] == '4'):\n",
    "        for filename in files:\n",
    "            if (filename == 'positive.txt'):\n",
    "                with open(os.path.join(directories, filename)) as f:\n",
    "                    for line in f:\n",
    "                        positive_words.append(line.strip())\n",
    "            if (filename == 'negative.txt'):\n",
    "                with open(os.path.join(directories, filename)) as f:\n",
    "                    for line in f:\n",
    "                        negative_words.append(line.strip())\n",
    "            if (filename == 'neutral.txt'):\n",
    "                with open(os.path.join(directories, filename), encoding='latin-1') as f:\n",
    "                    for line in f:\n",
    "                        neutral_words.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/posts_data.csv')\n",
    "data = df['Body']\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "\n",
    "for line in data:\n",
    "    tokens = gensim.utils.simple_preprocess(line)\n",
    "    text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stop words \n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "fil_stop = []\n",
    "with open('assets/stop_words_ph.txt') as f:\n",
    "    for line in f:\n",
    "        fil_stop.append(line.rstrip('\\n')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in text data: 34400\n",
      "Unique words: 5614\n",
      "Word sad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tired', 0.99920254945755),\n",
       " ('came', 0.9990960359573364),\n",
       " ('feels', 0.9990941882133484),\n",
       " ('think', 0.999048113822937),\n",
       " ('seriously', 0.9990304112434387),\n",
       " ('yet', 0.9990196228027344),\n",
       " ('family', 0.9989954233169556),\n",
       " ('told', 0.9989808797836304),\n",
       " ('words', 0.9989672899246216),\n",
       " ('ever', 0.9989591836929321),\n",
       " ('smile', 0.9989345073699951),\n",
       " ('heart', 0.9989292621612549),\n",
       " ('one', 0.9989150166511536),\n",
       " ('come', 0.9989084601402283),\n",
       " ('others', 0.9988980293273926),\n",
       " ('leave', 0.9988976716995239),\n",
       " ('cause', 0.9988868236541748),\n",
       " ('making', 0.9988847970962524),\n",
       " ('living', 0.9988800287246704),\n",
       " ('mom', 0.9988772869110107)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text_data = []\n",
    "counter = 0\n",
    "for line in data:\n",
    "    tokens = gensim.utils.simple_preprocess(line)\n",
    "    \n",
    "    # Remove Other ADMU FW mentions\n",
    "    tokens = [re.sub('\\#ADMUFreedomWall\\ \\d+', '', sent) for sent in tokens]\n",
    "    tokens =  [re.sub('\\#ADMUFreedomWall\\d+', '', sent) for sent in tokens]\n",
    "    tokens = [word for word in tokens if word not in fil_stop]\n",
    "    tokens = [word for word in tokens if word not in en_stop]\n",
    "    \n",
    "    test_text_data.append(tokens)\n",
    "    counter += len(tokens)\n",
    "\n",
    "print('Words in text data:', counter)\n",
    "# Number of unique words\n",
    "numbers_set = set(i for j in test_text_data for i in j)\n",
    "print('Unique words:', len(numbers_set))\n",
    "        \n",
    "model = gensim.models.Word2Vec(test_text_data, size=100, window=5, \n",
    "                               min_count=10, workers=4)\n",
    "\n",
    "model.train(test_text_data, total_examples=model.corpus_count,epochs=10)\n",
    "\n",
    "word = 'sad'\n",
    "print('Word', word)\n",
    "model.wv.most_similar(word, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540608, 680820)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(text_data, size=100, window=10, min_count=2, workers=4)\n",
    "model.train(text_data,total_examples=len(text_data),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ayun', 0.9957599639892578),\n",
       " ('duper', 0.9957374930381775),\n",
       " ('classmate', 0.9957038164138794),\n",
       " ('markets', 0.9952459335327148),\n",
       " ('kuwentuhan', 0.995223879814148),\n",
       " ('youth', 0.9949400424957275),\n",
       " ('plzzz', 0.994774580001831),\n",
       " ('permanent', 0.9938199520111084),\n",
       " ('shorts', 0.9935779571533203),\n",
       " ('namee', 0.9935511350631714)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"cute\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "# sentences and corpus from standard library\n",
    "sentences = list(itertools.islice(Text8Corpus('text8'),None))\n",
    "corpus = Corpus()\n",
    "\n",
    "# fitting the corpus with sentences and creating Glove object\n",
    "corpus.fit(sentences, window=10)\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "\n",
    "# fitting to the corpus and adding standard dictionary to the object\n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
